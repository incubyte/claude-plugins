# Spec: /bee:architect Phase 1 -- Domain-Grounded Assessment with Runnable Tests

## Overview

The `/bee:architect` command assesses architectural health by comparing a codebase's structure against its domain language -- extracted from README, docs, the company's website, and code naming patterns. It produces a developer-reviewed assessment report, then generates runnable ArchUnit-style test files that document good boundaries (passing tests) and flag architecture leaks (intentionally failing tests).

## Acceptance Criteria

### Command Orchestration (commands/architect.md)

- [x]Developer invokes `/bee:architect` and the command spawns context-gatherer, review-coupling, review-behavioral, and domain-language-extractor agents in parallel
- [x]If any agent fails or times out, the command notes which analysis dimension was skipped and continues with remaining results
- [x]The command merges findings from all four agents into a single architecture assessment report
- [x]The command asks the developer 2-3 targeted validation questions based on the merged findings (e.g., "Is Orders really separate from Shipments in your domain?")
- [x]Validation questions use AskUserQuestion with concrete options derived from the findings, not open-ended prompts
- [x]After validation, the command saves the assessment report to `docs/architecture-assessment.md`
- [x]The command runs the collaboration loop on the assessment report (developer can add `@bee` annotations, marks `[x] Reviewed` to proceed)
- [x]Once the report is reviewed, the command delegates to the architecture-test-writer agent to generate test files
- [x]The command reports a summary of what was generated: how many tests, where they live, how many are expected to pass vs fail

### Domain Language Extractor Agent (agents/domain-language-extractor.md)

- [x]The agent reads README, docs/, and API description files (OpenAPI specs, route definitions) to extract domain vocabulary
- [x]The agent uses WebFetch to visit the company's website and extract marketing language describing the product's domain
- [x]If no website is found or WebFetch fails, the agent asks the developer for the URL or key domain terms
- [x]As fallback, the agent infers domain vocabulary from code naming patterns (module names, class names, function names, directory structure)
- [x]The agent produces a structured domain vocabulary: a list of domain concepts with where each was found (docs, website, code, developer input)
- [x]The agent compares domain vocabulary against code structure and flags two types of mismatches: vocabulary drift (domain says "Order", code says "transaction") and boundary violations (concepts the domain treats as separate that are tangled in one module)
- [x]The agent does not modify any files -- read-only analysis

### Architecture Test Writer Agent (agents/architecture-test-writer.md)

- [x]The agent receives the confirmed assessment report and the context-gatherer's test infrastructure details
- [x]The agent auto-detects the project's test framework from context-gatherer output (Jest, Vitest, Pytest, RSpec, JUnit, etc.)
- [x]If the test framework cannot be detected, the agent asks the developer which framework to use
- [x]The agent generates module-level boundary tests: which modules should exist, which dependencies are allowed between them, which domain concepts belong in which modules
- [x]Tests that validate existing good boundaries are written to pass
- [x]Tests that flag architecture leaks (detected mismatches from the report) are written to intentionally fail
- [x]The agent places test files in an `architecture/` subfolder within the project's detected test directory (e.g., `tests/architecture/`, `__tests__/architecture/`, `test/architecture/`)
- [x]Each generated test file includes a comment header explaining that it was generated by `/bee:architect` and what it validates
- [x]The agent does not modify any existing test files -- it only creates new files in the architecture subfolder

### Assessment Report Format

- [x]The report starts with a domain vocabulary section listing extracted concepts and their sources
- [x]The report includes a boundary map showing which modules own which domain concepts
- [x]The report includes a mismatches section with vocabulary drift items and boundary violation items
- [x]The report includes a "healthy boundaries" section documenting what is working well
- [x]Each mismatch item includes a plain-language explanation of why it matters
- [x]The report ends with a `[ ] Reviewed` checkbox for the collaboration loop

### Error Handling

- [x]If the project has no README and no docs and no detectable website, the command falls back to code-only analysis and asks the developer for domain vocabulary
- [x]If the project has no test framework detected and the developer does not specify one, the command produces the assessment report but skips test generation with a clear message
- [x]If the project has no git history, the review-behavioral agent is skipped and the command notes this in the report

## Assessment Report Shape

```markdown
# Architecture Assessment: [project name]

## Domain Vocabulary
| Concept | Source | Code Match |
|---------|--------|------------|
| Order | README, website | `orders/` module |
| Shipment | website | (not found -- tangled in `orders/`) |

## Boundary Map
- `orders/` -- owns: Order, LineItem
- `payments/` -- owns: Payment, Invoice

## Healthy Boundaries
- [what is working well, worth preserving]

## Mismatches

### Vocabulary Drift
- Domain says "Shipment", code says "delivery" in `orders/utils.ts`

### Boundary Violations
- Order and Shipment logic both live in `orders/service.ts` -- domain treats these as separate concepts

## Validation Notes
- Developer confirmed: Orders and Shipments are separate bounded contexts
- Developer confirmed: "delivery" should be renamed to "shipment"

[ ] Reviewed
```

## Out of Scope

- Phase 2: greenfield integration with `/bee:build` (architecture tests from discovery's module map before feature code)
- Class-level or function-level naming checks (Phase 1 is module-level boundaries only)
- Fixing the violations found -- the command is read-only plus test generation
- Runtime architecture analysis (only static analysis and git history)
- Full domain-driven design modeling or event storming facilitation
- CI integration or continuous monitoring -- this is an on-demand assessment
- Generating tests for frameworks not detectable from project config (developer specifies manually as fallback)
- Re-running architecture tests after each phase ships (future consideration)

## Technical Context

- **Pattern to follow**: Orchestrator-Agent, matching `commands/review.md` for parallel agent spawning and `commands/build.md` for the collaboration loop and multi-step flow
- **New files to create**: `commands/architect.md`, `agents/domain-language-extractor.md`, `agents/architecture-test-writer.md`
- **Existing agents to reuse**: `agents/context-gatherer.md` (codebase scan + test infrastructure), `agents/review-coupling.md` (boundary violations, import analysis), `agents/review-behavioral.md` (hotspots, temporal coupling)
- **Agent constraints**: Claude Code subagents cannot spawn other subagents. Only the command orchestrator can use the Task tool to spawn agents. Agents use Read, Glob, Grep, Bash, WebFetch.
- **YAML frontmatter**: All agent files need `name`, `description`, `tools` fields in frontmatter (see `agents/context-gatherer.md` for pattern)
- **Skills reference**: Agents should read `skills/architecture-patterns/SKILL.md` and `skills/clean-code/SKILL.md` for domain boundary and dependency direction knowledge
- **Risk level**: LOW (internal developer tooling)

[x] Reviewed
