# TDD Plan: Bee Architect Phase 1 -- Slice 2: Architecture Test Writer Agent

## Execution Instructions
Read this plan. Work on every item in order.
Mark each checkbox done as you complete it ([ ] -> [x]).
Continue until all items are done.
If stuck after 3 attempts, mark with a warning and move to the next independent step.

## Context
- **Source**: `docs/specs/bee-architect-phase-1.md`
- **Slice**: Architecture Test Writer Agent (`agents/architecture-test-writer.md`)
- **Risk**: LOW
- **File to create**: `agents/architecture-test-writer.md`
- **Acceptance Criteria**:
  1. Receives the confirmed assessment report and context-gatherer's test infrastructure details
  2. Auto-detects the project's test framework from context-gatherer output (Jest, Vitest, Pytest, RSpec, JUnit, etc.)
  3. If test framework cannot be detected, asks the developer which framework to use
  4. Generates module-level boundary tests: which modules should exist, which dependencies are allowed, which domain concepts belong in which modules
  5. Tests that validate existing good boundaries are written to PASS
  6. Tests that flag architecture leaks (detected mismatches) are written to intentionally FAIL
  7. Places test files in an `architecture/` subfolder within the project's detected test directory
  8. Each generated test file includes a comment header explaining it was generated by `/bee:architect`
  9. Does NOT modify existing test files -- only creates new files in the architecture subfolder
  10. Tools: Read, Write, Glob, Grep

## Codebase Analysis

### File Structure
- Implementation: `agents/architecture-test-writer.md` (new file)
- Pattern reference: `agents/context-gatherer.md`, `agents/review-coupling.md`, `agents/domain-language-extractor.md`
- Skills to reference: `skills/architecture-patterns/SKILL.md`, `skills/clean-code/SKILL.md`, `skills/tdd-practices/SKILL.md`

### Agent Pattern
All agents follow the same structure:
1. YAML frontmatter: `name`, `description`, `tools`, `model: inherit`
2. One-line persona statement
3. Skills section (read skill files for reference)
4. Inputs section (what the agent receives)
5. Process section (numbered steps)
6. Output Format section (markdown template)
7. Rules section (constraints)

### Verification Method
Since this is a markdown agent definition (no compiled code, no test framework), each step is verified by reading the file back and confirming the section exists, follows the established pattern, and covers the relevant acceptance criteria.

---

## Behavior 1: YAML frontmatter and persona

**Given** the established agent pattern (frontmatter + persona line)
**When** the architecture-test-writer agent file is created
**Then** it has valid YAML frontmatter with name, description, tools (Read, Write, Glob, Grep), model: inherit, and a one-line persona

- [x] **DEFINE**: Frontmatter needs:
  - `name: architecture-test-writer`
  - `description`: one sentence about generating architecture boundary tests from an assessment report
  - `tools: Read, Write, Glob, Grep` (this agent writes files, unlike the read-only agents)
  - `model: inherit`
  - Persona line: something like "You are an architecture test generator..."

- [x] **APPLY**: Create `agents/architecture-test-writer.md` with frontmatter and persona line only.

- [x] **VERIFY**: Read the file. Confirm frontmatter has all four fields. Confirm `tools` includes Write (unique to this agent vs the analysis agents). Confirm persona line exists after the closing `---`.

---

## Behavior 2: Skills section

**Given** the agent needs architecture boundary knowledge, clean code naming, and TDD practices
**When** the skills section is written
**Then** it references architecture-patterns, clean-code, and tdd-practices skill files

- [x] **DEFINE**: A Skills section that instructs the agent to read:
  - `skills/architecture-patterns/SKILL.md` -- for module boundary and dependency direction knowledge
  - `skills/clean-code/SKILL.md` -- for naming conventions in generated tests
  - `skills/tdd-practices/SKILL.md` -- for test quality guidance (one assertion per test, clear names)

- [x] **APPLY**: Add the Skills section after the persona line.

- [x] **VERIFY**: Read the file. Confirm all three skill paths are listed. Confirm the section follows the pattern from other agents.

---

## Behavior 3: Inputs section

**Given** AC1: the agent receives the confirmed assessment report and context-gatherer output
**When** the inputs section is written
**Then** it documents what the agent receives: assessment report and test infrastructure details

- [x] **DEFINE**: The agent receives:
  - **assessment_report**: the confirmed assessment report (from `docs/architecture-assessment.md`) containing domain vocabulary, boundary map, healthy boundaries, and mismatches
  - **test_infrastructure**: the context-gatherer's test infrastructure section (framework, location, naming convention, run command)
  - **project_root**: path to the project

- [x] **APPLY**: Add the Inputs section.

- [x] **VERIFY**: Read the file. Confirm both assessment_report and test_infrastructure are documented as inputs. Confirm it matches the style of other agents' Inputs sections.

---

## Behavior 4: Process step -- detect test framework

**Given** AC2 and AC3: auto-detect test framework, ask if undetectable
**When** the framework detection step is written
**Then** it instructs the agent to read framework from context-gatherer output, with fallback to asking the developer

- [x] **DEFINE**: Process step 1 should:
  - Read the test infrastructure details from context-gatherer output to identify framework (Jest, Vitest, Pytest, RSpec, JUnit, Go test, etc.)
  - Also identify the test directory location and naming convention
  - If framework is not present or unclear in the context-gatherer output, use AskUserQuestion with concrete options (e.g., "Jest", "Vitest", "Pytest", "Other -- please specify")
  - Store the detected framework, test directory, and naming convention for use in subsequent steps

- [x] **APPLY**: Add Process section with step 1.

- [x] **VERIFY**: Read step 1. Confirm it references context-gatherer output as the primary source. Confirm the AskUserQuestion fallback offers concrete framework choices, not open-ended prompts.

---

## Behavior 5: Process step -- determine test output location

**Given** AC7: place tests in an `architecture/` subfolder within the detected test directory
**When** the location determination step is written
**Then** it instructs the agent to resolve the target directory path

- [x] **DEFINE**: Process step 2 should:
  - Use the detected test directory from step 1 (e.g., `tests/`, `__tests__/`, `test/`, `spec/`)
  - Create the `architecture/` subfolder path within it (e.g., `tests/architecture/`, `__tests__/architecture/`)
  - If no test directory is detected, fall back to a sensible default based on the framework

- [x] **APPLY**: Add step 2 to the Process section.

- [x] **VERIFY**: Read step 2. Confirm it builds the architecture subfolder path from the detected test directory. Confirm it gives examples of common patterns.

---

## Behavior 6: Process step -- generate passing boundary tests

**Given** AC4 and AC5: generate module-level boundary tests; good boundaries pass
**When** the passing test generation step is written
**Then** it instructs the agent to generate tests for healthy boundaries from the assessment report

- [x] **DEFINE**: Process step 3 should:
  - Read the "Healthy Boundaries" and "Boundary Map" sections of the assessment report
  - For each healthy boundary, generate a test that validates: the module exists, it owns the expected domain concepts, its dependencies are within the allowed set
  - These tests are written to PASS against the current codebase -- they document what is good
  - Use the detected framework's syntax (describe/it for Jest/Vitest, def test_ for Pytest, etc.)
  - Each test should have a descriptive name that reads as documentation (e.g., "orders module should exist", "payments should not import from orders internals")

- [x] **APPLY**: Add step 3 to the Process section.

- [x] **VERIFY**: Read step 3. Confirm it sources from the healthy boundaries in the report. Confirm tests are described as passing. Confirm it mentions framework-appropriate syntax.

---

## Behavior 7: Process step -- generate failing architecture leak tests

**Given** AC6: tests that flag mismatches are intentionally failing
**When** the failing test generation step is written
**Then** it instructs the agent to generate tests that expose architecture leaks from the mismatches section

- [x] **DEFINE**: Process step 4 should:
  - Read the "Mismatches" section (vocabulary drift + boundary violations) of the assessment report
  - For each mismatch, generate a test that asserts the desired state (the fix), which will FAIL against the current codebase
  - Each failing test should include a comment explaining: what the mismatch is, why it matters, and what the fix would look like
  - Mark failing tests clearly (e.g., with a `// EXPECTED TO FAIL` comment or a descriptive test name like "FIXME: shipment logic should not live in orders module")
  - Do not use test framework skip/pending markers -- the tests should run and fail visibly

- [x] **APPLY**: Add step 4 to the Process section.

- [x] **VERIFY**: Read step 4. Confirm it sources from the mismatches section. Confirm tests are described as intentionally failing. Confirm it instructs the agent to add explanatory comments to each failing test.

---

## Behavior 8: Process step -- write test files with comment headers

**Given** AC8 and AC9: each file includes a generated-by comment header, no existing files modified
**When** the file writing step is written
**Then** it instructs the agent to write the test files with a comment header explaining provenance

- [x] **DEFINE**: Process step 5 should:
  - Write each test file to the `architecture/` subfolder determined in step 2
  - Every generated file starts with a comment header (language-appropriate: `//`, `#`, `/* */`) explaining:
    - Generated by `/bee:architect`
    - What this file validates (e.g., "Module boundary tests for the orders domain")
    - That passing tests document healthy boundaries; failing tests flag architecture leaks to fix
  - Use the Write tool to create each file
  - Organize tests logically: one file per module boundary or one file per concern (not one giant file)
  - NEVER modify existing test files -- only create new files

- [x] **APPLY**: Add step 5 to the Process section.

- [x] **VERIFY**: Read step 5. Confirm it specifies the comment header content. Confirm it uses the Write tool. Confirm the no-modify-existing-files constraint is stated here as well as in Rules.

---

## Behavior 9: Output format

**Given** the orchestrator needs a summary of what was generated
**When** the output format section is written
**Then** it provides a markdown template summarizing the generated test files

- [x] **DEFINE**: Output format should include:
  - List of files created with their paths
  - Count of passing tests (healthy boundaries)
  - Count of failing tests (architecture leaks)
  - The run command to execute the tests
  - Brief explanation of what passing vs failing means in this context

- [x] **APPLY**: Add the Output Format section.

- [x] **VERIFY**: Read the output format. Confirm it has file list, counts, run command, and the pass/fail explanation. Confirm the orchestrator can use this summary directly in its final report.

---

## Behavior 10: Rules section

**Given** AC9: does not modify existing test files, plus agent constraints
**When** the rules section is written
**Then** it lists all constraints: no modifying existing files, no sub-agents, write-only to architecture subfolder

- [x] **DEFINE**: Rules should include:
  - Do NOT modify any existing test files -- only create new files in the architecture/ subfolder
  - Do not spawn sub-agents
  - If the assessment report has no mismatches, generate only passing tests and note that the architecture looks clean
  - If the assessment report has no healthy boundaries, generate only failing tests and note that
  - Generated tests must be runnable as-is -- no placeholder code or TODOs
  - If no test framework is detected and the developer declines to specify one, exit gracefully with a clear message (do not generate tests)

- [x] **APPLY**: Add the Rules section.

- [x] **VERIFY**: Read the rules. Confirm the no-modify-existing-files constraint is present. Confirm no-sub-agents constraint is present. Confirm graceful handling of edge cases (empty mismatches, empty healthy boundaries, no framework).

---

## Edge Cases (LOW risk -- minimal)

- [x] **VERIFY**: The tools list in frontmatter includes Write (this agent creates files) but does NOT include WebFetch or Bash.

- [x] **VERIFY**: The process handles the case where no test framework is detected AND the developer declines to specify one -- the agent should produce a clear message and skip test generation (per the Error Handling AC in the spec).

- [x] **VERIFY**: The file follows valid markdown structure -- frontmatter fences (`---`), consistent heading levels, no broken formatting.

---

## Final Check

- [x] Read `agents/architecture-test-writer.md` top to bottom. Confirm:
  - YAML frontmatter has name, description, tools (Read, Write, Glob, Grep), model: inherit
  - Persona line sets the agent's role clearly
  - Skills section references architecture-patterns, clean-code, and tdd-practices
  - Inputs section documents assessment_report, test_infrastructure, and project_root
  - Process has 5 steps: detect framework, determine location, generate passing tests, generate failing tests, write files with headers
  - Output format gives the orchestrator a usable summary
  - Rules enforce no-modify-existing, no sub-agents, runnable tests, graceful degradation
  - File reads naturally from top to bottom

## Summary
| Step | Description | Status |
|------|------------|--------|
| Behavior 1 | YAML frontmatter and persona | done |
| Behavior 2 | Skills section | done |
| Behavior 3 | Inputs section | done |
| Behavior 4 | Process -- detect test framework | done |
| Behavior 5 | Process -- determine test output location | done |
| Behavior 6 | Process -- generate passing boundary tests | done |
| Behavior 7 | Process -- generate failing leak tests | done |
| Behavior 8 | Process -- write files with comment headers | done |
| Behavior 9 | Output format | done |
| Behavior 10 | Rules section | done |
| Edge cases | Tools, framework fallback, valid markdown | done |
| Final check | Full file review | done |

---

[x] Reviewed
